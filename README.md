# Mitigating Unintended Identity Bias in Toxicity Detection
Machine Learning models are commonly used in online platforms to detect toxicity and ensure a civil and safe environment for citizens. These models are trained on datasets annotated by human raters. However, they tend to be biased toward certain identities leading to unfair application and therefore potentially excluding them from online conversations by suffocating their voices. This project shows that certain identities, in particular LGBTQ+ and black communities, are affected by this unintended bias. It studies the role input data plays in this bias and shows how balancing the training data with respect to discriminated identities can mitigate the unintended bias without compromising the performance of the model.

This work is what I conducted in a course project in Trustworthy ML course at HEC Montreal in the Fall of 2022. 
